---
layout: post
title: High dimensional probability 
author: "Karthik"
categories: journal
tags: [documentation,sample]
---

**Ref**: Prof. Vershynin's [handwritten notes](https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html)

**Lec-1**: 

Consider the problem of numerically computing the integral of an ${ f : [0,1] ^d \to \mathbb{R} }.$ 

Breaking ${ [0,1] ^d }$ into (axis aligned) cubes of width ${ \epsilon },$ there are about ${ N \approx (\frac{1}{\epsilon}) ^{d} }$ many such smaller cubes. Now the integral ${ \int _{[0,1] ^d} f(x) dx \approx \frac{1}{N} \sum _{1} ^{N} f(x _i) }$ where the ${ N }$ many ${ x _i }$s are each points from the ${ N }$ smaller cubes. 

In the Monte-Carlo method, instead of having ${ x _i }$s taken from the smaller cubes, they are considered as random variables ${ X _1, \ldots, X _N \stackrel{iid}{\sim} \text{Unif}[0,1] ^d }$ and the random variable ${ \frac{1}{N} \sum _{1} ^{N} f(X _i) }$ approximates (i.e. is an estimator of) ${ \int _{[0,1] ^d} f (x) dx }.$   
Notice expectation ${ \mathbb{E}[\frac{1}{N} \sum _{1} ^{N} f (X _i)] }$ ${ = \mathbb{E}[f(X)] = \int _{[0,1] ^d} f(x) dx }$ where ${ X \sim \text{Unif}[0,1]  ^{d} }.$ Also, the mean squared error ${ \mathbb{E}[ \left( \frac{1}{N} \sum _{1} ^{N} f(X _i) - \int _{[0,1] ^d} f(x) dx \right) ^2]  }$ ${ = \text{var} \left( \frac{1}{N} \sum _{1} ^{N} f (X _i) \right) }$ ${ = \frac{1}{N ^2} \text{var}(\sum _{1} ^{N} f (X _i) ) }$ ${ = \frac{1}{N} \text{var}(f(X)) },$ which is ${ \leq \frac{1}{N} \int f ^{2} (X) dx }$ ${ \leq \frac{1}{N} \lVert f \rVert _{\infty} ^{2} }.$ So standard deviation (root of mean squared error) of the estimator is ${ O(\frac{1}{\sqrt{N}}) }.$ 

---

**Lec-2**: 

A set ${ T \subseteq \mathbb{R} ^d }$ is convex if for all ${ x, y \in T }$ the segment ${ [x,y] = \lbrace x + t(y-x) : t \in [0,1] \rbrace \subseteq T }.$

Intersection of any family of convex sets is convex. For ${ T \subseteq \mathbb{R} ^d }$ its convex hull is ${ \text{conv}(T) = \cap \lbrace \text{convex sets containing } T \rbrace }$ (the smallest convex set containing ${ T }$).  

Obs: Let ${ T \subseteq \mathbb{R} ^d }.$ Now ${ \text{conv}(T) = \lbrace \text{convex combinations of } z _1, \ldots, z _n \in T \text{ for } n \in \mathbb{Z} _{> 0} \rbrace }$   
(A convex combination of ${ z _1, \ldots, z _n \in \mathbb{R} ^d }$ is an expression of the form ${ \sum _{1} ^{n} \lambda _i z _i }$ where ${ \lambda _i \geq 0, \sum _{1} ^{n} \lambda _i = 1 }$)   
Sketch: For the ${ T }$ finite case, that ${ \text{conv}(\lbrace p _1, \ldots, p _n \rbrace) = \lbrace \sum _{1} ^{n} \lambda _i p _i : \text{each } \lambda _i \geq 0, \sum _{1} ^{n} \lambda _i = 1 \rbrace }$ is shown inductively. 

**Thm** [Caratheodory]: Let ${ T \subseteq \mathbb{R} ^d },$ and ${ z \in \text{conv}(T) }.$ Consider the smallest ${ m }$ such that ${ z }$ can be written as a convex combination of some ${ m }$ points in ${ T }.$ Now ${ m \leq d + 1 }.$   
**Pf**: [Link](https://planetmath.org/proofofcaratheodorystheorem). Since ${ z \in \text{conv}(T) }$ it can be written as ${ z = \sum _{i=1} ^{n} \lambda _i z _i }$ with each ${ \lambda _i \geq 0 ,}$ ${ \sum _{i=1} ^{n} \lambda _i = 1 }.$ If ${ n \leq d +1 }$ we are done, so suppose ${ n > d + 1 }.$ So ${ z _2 - z _1, \ldots, z _n - z _1 \in \mathbb{R} ^d }$ are linearly dependent. So ${ \sum _{i=2} ^{n} \beta _i (z _i - z _1) = 0 }$ for some ${ \beta _i }$s not all ${ 0 }.$ Equivalently ${ \sum _{i=1} ^{n} \gamma _i z _i = 0 }$ with ${ \sum _{i=1} ^{n} \gamma _i = 0 }$ and not all ${ \gamma _i }$ zero. The equations we now have are $${ z = \sum _{i=1} ^{n} \lambda _i z _i , \lambda _i \geq 0, \sum _{1} ^{n} \lambda _i = 1, }$$ $${ \sum _{i =1} ^{n} \gamma _i z _i = 0, \sum _{i=1} ^{n} \gamma _i = 0, \text{ and not all } \gamma _i \text{ are } 0 }.$$    
Focus on the indices ${ \mathscr{I} = \lbrace i \in \lbrace 1 , \ldots, n \rbrace : \gamma _i > 0 \rbrace  },$ a nonempty set. The quantity ${ \lambda = \max _{i \in \mathscr{I}} \frac{\lambda _i}{\gamma _i} }$ lets us write ${  z = \sum _{i =1} ^{n} (\lambda _i - \lambda \gamma _i) z _i  ,}$ and notice this is a convex combination as well. This new convex combination has atleast one coefficient ${ 0 }.$ So roughly, we showed whenever ${ z }$ is a convex combination of ${ n > d + 1 }$ points we can write it as a convex combination with one lesser point, as needed. 

**Thm** [Approx Caratheodory]: Let ${ T \subseteq \mathbb{R} ^{d} }$ with ${ \text{diam}(T) < \infty }.$ Then for all ${ z \in \text{conv}(T) }$ and ${ k \in \mathbb{Z} _{> 0} ,}$ there exist ${ z _1, \ldots, z _k \in T }$ such that ${ \left\lVert z - \frac{1}{k} \sum _{i=1} ^{k} z _i \right\rVert _{2} \leq \frac{\text{diam}(T)}{\sqrt{2k}} .}$   
**Pf**: Recall the following fact about variances.   

> For a random variable ${ X },$ its variance ${ \mathbb{E}[(X - \mathbb{E}[X]) ^2] = \frac{1}{2} \mathbb{E}[(X - X ^{'}) ^2] }$ where ${ X ^{'} }$ is an i.i.d copy of ${ X }.$ The proof is by expanding RHS. Similarly, for a random vector ${ Z }$ we have ${ \mathbb{E}\lVert Z - \mathbb{E}[Z] \rVert ^2 = \frac{1}{2} \mathbb{E}\lVert Z - Z ^{'} \rVert ^2  }$ where ${ Z ^{'} }$ is an i.i.d copy of ${ Z }.$ 
  
Since ${ z \in \text{conv}(T) }$ we can write ${ z = \sum _{i=1} ^{n} \lambda _i z _i  }$ with ${ \lambda _i \geq 0 ,}$ ${ \sum _{i=1} ^{n} \lambda _i = 1 ,}$ ${ z _i \in T }.$   
Let ${ Z }$ be a random vector taking value ${ z _i }$ with probability ${ \lambda _i }.$ (So especially ${ \mathbb{E}[Z] = z }$). Consider its i.i.d copies ${ Z, Z _1, Z _2, \ldots }$ By SLLN (Strong Law of Large Numbers), ${ \frac{1}{k} \sum _{i=1} ^{k} Z _i \stackrel{a.s}{\longrightarrow} \mathbb{E}[Z] = z }.$ Looking at the error, ${ \mathbb{E} \lVert z - \frac{1}{k} \sum _{i=1} ^{k} Z _i \rVert ^{2} }$ ${ = \mathbb{E} \lVert \frac{1}{k} \sum _{i=1} ^{k} (z - Z _i) \rVert ^{2} }$ ${ = \frac{1}{k ^2} \mathbb{E} \lVert \sum _{i=1} ^{k} (z - Z _i) \rVert ^2 }.$ 

> For random vectors ${ A, B \in \mathbb{R} ^{d} }$ note ${ \mathbb{E}[\lVert A + B \rVert ^2] }$ ${ = \mathbb{E}[\lVert A \rVert ^2] + \mathbb{E}[\lVert B \rVert ^2] + 2 \mathbb{E}[A \cdot B] }.$ When say ${ A = (Z _1 - \mathbb{E}[Z _1]) }$ and ${ B = (Z _2 - \mathbb{E}[Z _2]) }$ where ${ Z _1, Z _2 }$ are i.i.d vectors, the dot product has expectation ${ \mathbb{E}[A \cdot B] = 0 }$ and hence ${ \mathbb{E}[\lVert A + B \rVert ^2] = \mathbb{E}[\lVert A \rVert ^2] + \mathbb{E}[\lVert B \rVert ^2] }$ holds.  

So ${ \mathbb{E} \lVert z - \frac{1}{k} \sum _{i=1} ^{k} Z _i \rVert ^{2} = \frac{1}{k} \mathbb{E}\lVert z - Z \rVert ^2 }$ ${ = \frac{1}{2k} \mathbb{E} \lVert Z ^{'} - Z \rVert ^2 }$ ${ \leq \frac{\text{diam}(T) ^2}{2k} }.$   
So there exist realisations ${ z _1, \ldots, z _k }$ of ${ Z _1, \ldots, Z _k }$ for which ${ \lVert z - \frac{1}{k} \sum _{i=1} ^{k} z _i \rVert ^{2} \leq \frac{\text{diam}(T) ^2}{2k} , }$ as needed. 










